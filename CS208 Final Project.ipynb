{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(208)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph implementation and related core graph algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Implementation of n-ary trees (for use in generating random trees to test various DP algorithms on)\n",
    "######################################################################################################\n",
    "\n",
    "\n",
    "# Generic tree node class\n",
    "class TreeNode:\n",
    "    def __init__(self, weight=None, parent=None, depth=None, size=None):\n",
    "        # children of this node in tree\n",
    "        self.children = []\n",
    "        # weight of edge from parent to this node\n",
    "        self.weight = weight\n",
    "        # parent of this node in tree\n",
    "        self.parent = parent\n",
    "        # depth of this node in overall tree\n",
    "        self.depth = depth\n",
    "        # size of subtree rooted at this node\n",
    "        self.size = size\n",
    "        # heaviest child of this node\n",
    "        self.heavy_child = None\n",
    "\n",
    "        \n",
    "# Generates a complete n-ary tree with random integer weights in [0, B] and V vertices\n",
    "# Returns root node of generated tree\n",
    "def generate_complete_tree(n, B, V, depth=0, first_root=True):\n",
    "    if V == 1:\n",
    "        return TreeNode(weight=np.random.randint(0, B + 1), depth=depth, size=1)\n",
    "\n",
    "    if first_root:\n",
    "        curr = TreeNode(weight=0, depth=depth, size=V)\n",
    "    else:\n",
    "        curr = TreeNode(weight=np.random.randint(0, B + 1), depth=depth, size=V)\n",
    "    \n",
    "    # have V-1 > 0 vertices to split among n subtrees\n",
    "    left, j, each = V - 1, 0, 0\n",
    "    while left > n ** (j + 1):\n",
    "        each += n ** j\n",
    "        left -= n ** (j + 1)\n",
    "        j += 1\n",
    "    \n",
    "    for i in range(n):\n",
    "        newV = each + min(left, n ** j)\n",
    "        left -= min(left, n ** j)\n",
    "        \n",
    "        if newV > 0:\n",
    "            child = generate_complete_tree(n, B, newV, depth + 1, first_root=False)\n",
    "            child.parent = curr\n",
    "            curr.children.append(child)\n",
    "    \n",
    "    return curr\n",
    "\n",
    "# Generates a degenerate tree that is a path graph with random integer weights \n",
    "# in [0, B] and V vertices. Returns root node of the generated tree\n",
    "def generate_path_graph(n, B, V):\n",
    "    depth = 0\n",
    "    ptr = root = TreeNode(weight=0, depth=depth, size=V)\n",
    "    V -= 1\n",
    "    while V > 0:\n",
    "        depth += 1\n",
    "        child = TreeNode(weight=np.random.randint(0, B + 1), depth=depth, size=V)\n",
    "        ptr.children = [child]\n",
    "        child.parent = ptr\n",
    "        ptr = child\n",
    "        V -= 1\n",
    "    return root\n",
    "\n",
    "\n",
    "# Prints out level-by-level traversal of subtree rooted at node root\n",
    "def level_traversal(root):\n",
    "    levels = []\n",
    "    \n",
    "    def traverse(node):\n",
    "        if not node: return\n",
    "        if len(levels) <= node.depth:\n",
    "            levels.append([])\n",
    "        levels[node.depth].append((node.weight, node.parent.weight if node.parent else None, node.size))\n",
    "        for child in node.children:\n",
    "            traverse(child)\n",
    "            \n",
    "    traverse(root)\n",
    "    print(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns lowest common ancestor of u and v\n",
    "# Basic implementation of LCA for simplicity (i.e., not the most efficient)\n",
    "def LCA(u, v):\n",
    "    if u.depth < v.depth:\n",
    "        u, v = v, u\n",
    "    while u.depth > v.depth:\n",
    "        u = u.parent\n",
    "    while u != v:\n",
    "        u, v = u.parent, v.parent\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True all-pairs distances function and related algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true distances from all vertices to root v0\n",
    "def dfs_true_distances_root(node, d, curr_dist=0):\n",
    "    if not node: return\n",
    "    curr_dist += node.weight\n",
    "    d[node] = curr_dist\n",
    "    for child in node.children:\n",
    "        dfs_true_distances_root(child, d, curr_dist)\n",
    "    curr_dist -= node.weight\n",
    "\n",
    "# Compute true distances between all vertices and root in tree\n",
    "def true_distances_root(v0):\n",
    "    d = dict()\n",
    "    dfs_true_distances_root(v0, d)\n",
    "    return d\n",
    "\n",
    "# Computes distances between all pairs of vertices in tree given distances between all vertices and root\n",
    "def distances_all_pairs(v0, d_root):\n",
    "    # Use LCA to compute distances between all pairs of vertices\n",
    "    dd = dict()\n",
    "    nodes = list(d_root.keys())\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(i, len(nodes)):\n",
    "            dd[(nodes[j], nodes[i])] = dd[(nodes[i], nodes[j])] = d_root[nodes[i]] + d_root[nodes[j]] - 2 * d_root[LCA(nodes[i], nodes[j])]\n",
    "        \n",
    "    return dd\n",
    "\n",
    "# Compute true distances between all pairs of vertices in tree\n",
    "def true_distances_all_pairs(v0):\n",
    "    return distances_all_pairs(v0, true_distances_root(v0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic composition for private all-pairs distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Naive Algorithm 1: Basic composition for all-pairs distances in trees\n",
    "######################################################################################################\n",
    "\n",
    "def basic_distances_all_pairs(v0, V, epsilon):\n",
    "    d = true_distances_all_pairs(v0)\n",
    "    # Add appropriate Laplacian noise to all distances computed, and release\n",
    "    for pair in d:\n",
    "        d[pair] += np.random.laplace(loc=0, scale=V*V/epsilon)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic graph for private all-pairs distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Naive Algorithm 2: generate synthetic tree and return all-pairs distances on that\n",
    "######################################################################################################\n",
    "\n",
    "def dfs_synthetic_distances_all_pairs(node, prev, epsilon, synthetic, first_root=True):\n",
    "    if not node: return\n",
    "    if not first_root:\n",
    "        if synthetic:\n",
    "            prev[node] = node.weight\n",
    "            node.weight += np.random.laplace(loc=0, scale=1/epsilon)\n",
    "        else:\n",
    "            node.weight = prev[node]\n",
    "    for child in node.children:\n",
    "        dfs_synthetic_distances_all_pairs(child, prev, epsilon, synthetic, first_root=False)\n",
    "\n",
    "        \n",
    "def synthetic_distances_all_pairs(v0, V, epsilon):\n",
    "    prev = dict()\n",
    "    # Add Lap(1/epsilon) noise to each edge in tree, to release epsilon-DP synthetic tree\n",
    "    dfs_synthetic_distances_all_pairs(v0, prev, epsilon, synthetic=True)\n",
    "    # Compute true all-pairs distances on synthetic tree\n",
    "    d = true_distances_all_pairs(v0)\n",
    "    # Undo noise added to each edge \n",
    "    dfs_synthetic_distances_all_pairs(v0, prev, epsilon, synthetic=False)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sealfon Algorithm 1 for private all-pairs distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Sealfon Algorithm 1: for all-pairs distances in trees\n",
    "######################################################################################################\n",
    "\n",
    "# computes private distances between all vertices and root v0\n",
    "def alg1(v0, V, epsilon):\n",
    "    if V == 1: \n",
    "        return {v0: 0}\n",
    "\n",
    "    # find v* = ptr (splitting vertex of recursion)\n",
    "    path, ptr, pathlength = [], v0, 0\n",
    "    while not all([child.size <= V / 2 for child in ptr.children]):\n",
    "        for child in ptr.children:\n",
    "            if child.size > V / 2:\n",
    "                path.append(ptr)\n",
    "                ptr = child\n",
    "                pathlength += ptr.weight\n",
    "                break\n",
    "\n",
    "    assert (ptr.size > V / 2 and all([child.size <= V / 2 for child in ptr.children]))\n",
    "\n",
    "    d = dict()\n",
    "    d[ptr] = pathlength + np.random.laplace(loc=0, scale=np.log2(V) / epsilon)\n",
    "    d[v0] = 0\n",
    "    for child in ptr.children:\n",
    "        d[child] = d[ptr] + child.weight + np.random.laplace(loc=0, scale=np.log2(V) / epsilon)\n",
    "\n",
    "    # cut off subtree containing ptr to prepare for recursion\n",
    "    old_ptr_children = ptr.children\n",
    "    old_ptr_size = ptr.size\n",
    "    ptr.children = []\n",
    "    ptr.size = 1\n",
    "    for node in path:\n",
    "        node.size -= old_ptr_size - 1\n",
    "\n",
    "    # recursively compute distances to roots in subtrees\n",
    "    ds = [alg1(v, v.size, epsilon) for v in old_ptr_children]\n",
    "    for i in range(len(ds)):\n",
    "        for v in ds[i]:\n",
    "            d[v] = d[old_ptr_children[i]] + ds[i][v]\n",
    "\n",
    "    d0 = alg1(v0, v0.size, epsilon)\n",
    "    for v in d0:\n",
    "        d[v] = d[v0] + d0[v]\n",
    "\n",
    "    # glue subtree that we cut back onto the tree, so as to not modify input tree\n",
    "    ptr.children = old_ptr_children\n",
    "    ptr.size = old_ptr_size\n",
    "    for node in path:\n",
    "        node.size += old_ptr_size - 1\n",
    "\n",
    "    # return private distances between all vertices and root v0\n",
    "    return d\n",
    "\n",
    "    \n",
    "def sealfon_distances_all_pairs(v0, V, epsilon):\n",
    "    # use above computed distances to root v0 to compute all-pairs distances\n",
    "    return distances_all_pairs(v0, alg1(v0, V, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fan and Li Algorithms 1/2 for private all-pairs distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "# Fan and Li Algorithms 1 + 2: for heavy path decomposition used for all-pairs distances in trees\n",
    "######################################################################################################\n",
    "\n",
    "def tree_decomposition(T):\n",
    "    # list of roots of subtrees (whose edge from their parent isn't heavy)\n",
    "    R = []\n",
    "    # list of nodes that are the heads of heavy paths in the tree T\n",
    "    S = [T]\n",
    "    ptr = T\n",
    "    while ptr.children:\n",
    "        maxChildSize = max([child.size for child in ptr.children])\n",
    "        heavy_candidates = [child for child in ptr.children if child.size == maxChildSize]\n",
    "        ptr.heavy_child = np.random.choice(heavy_candidates)\n",
    "        \n",
    "        for child in ptr.children:\n",
    "            if child is not ptr.heavy_child:\n",
    "                R.append(child)\n",
    "        ptr = ptr.heavy_child\n",
    "        \n",
    "    for v in R:\n",
    "        S += tree_decomposition(v)\n",
    "    return S\n",
    "    \n",
    "\n",
    "def dfs_fanli_distances_all_pairs(node, nodes, prev, epsilon, add_noise=True):\n",
    "    if not node: return\n",
    "    nodes.append(node)\n",
    "    for child in node.children:\n",
    "        if child is not node.heavy_child:\n",
    "            if add_noise:\n",
    "                prev[child] = child.weight\n",
    "                child.weight += np.random.laplace(loc=0, scale=1/epsilon)\n",
    "            else:\n",
    "                child.weight = prev[child]\n",
    "    for child in node.children:\n",
    "        dfs_fanli_distances_all_pairs(child, nodes, prev, epsilon, add_noise)\n",
    "        \n",
    "\n",
    "# returns the path length from u up to LCA (of u with some other vertex v)\n",
    "def path_length(u, dd, lca):\n",
    "    dist = 0\n",
    "    ptr = u\n",
    "    while ptr is not lca:\n",
    "        start = ptr\n",
    "        while ptr is not lca and ptr.parent and ptr.parent.heavy_child is ptr:\n",
    "            ptr = ptr.parent\n",
    "        dist += dd[(start, ptr)]\n",
    "        if ptr is not lca and ptr.parent:\n",
    "            dist += ptr.weight\n",
    "            ptr = ptr.parent\n",
    "    return dist \n",
    "\n",
    "\n",
    "def fanli_distances_all_pairs(T, V, epsilon):\n",
    "    dd = dict()\n",
    "    # list of heads of heavy paths in T\n",
    "    S = tree_decomposition(T)\n",
    "    for head in S:\n",
    "        # isolate heavy path from rest of tree, to pass into sealfon_distances_all_pairs \n",
    "        # to compute all-pairs distances within this heavy path only\n",
    "        new_size = 1\n",
    "        old_sizes, old_children = [], []\n",
    "        \n",
    "        ptr = head\n",
    "        while ptr.heavy_child:\n",
    "            old_children.append(ptr.children)\n",
    "            old_sizes.append(ptr.size)\n",
    "            ptr.children = [ptr.heavy_child]\n",
    "            ptr = ptr.heavy_child\n",
    "            new_size += 1\n",
    "        \n",
    "        ptr = head\n",
    "        while ptr.heavy_child:\n",
    "            ptr.size = new_size\n",
    "            ptr = ptr.heavy_child\n",
    "            new_size -= 1\n",
    "        \n",
    "        d = sealfon_distances_all_pairs(head, head.size, epsilon)\n",
    "        dd = {**dd, **d}\n",
    "        \n",
    "        # glue heavy path back onto rest of tree, so as to not modify input tree\n",
    "        while ptr.parent and ptr.parent.heavy_child is ptr:\n",
    "            ptr = ptr.parent\n",
    "            ptr.size = old_sizes.pop()\n",
    "            ptr.children = old_children.pop()\n",
    "    \n",
    "    nodes = []\n",
    "    prev = dict()\n",
    "    \n",
    "    # add noise to light edges\n",
    "    dfs_fanli_distances_all_pairs(T, nodes, prev, epsilon, add_noise=True)      \n",
    "    \n",
    "    # compute remaining all-pairs distances of nodes involving multiple heavy paths\n",
    "    for i in range(len(nodes) - 1):\n",
    "        for j in range(i + 1, len(nodes)):\n",
    "            # if distance between these 2 nodes isn't already computed within heavy paths, compute it\n",
    "            if (nodes[i], nodes[j]) not in dd:\n",
    "                lca = LCA(nodes[i], nodes[j])      \n",
    "                # add paths from nodes[i] up to lca and from nodes[j] up to lca\n",
    "                dd[(nodes[i], nodes[j])] = dd[(nodes[j], nodes[i])] = path_length(nodes[i], dd, lca) + path_length(nodes[j], dd, lca)\n",
    "                \n",
    "    # remove noise from light edges, so as to not modify input tree\n",
    "    dfs_fanli_distances_all_pairs(T, nodes, prev, epsilon, add_noise=False)\n",
    "    \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example tree and application of all-pairs distances algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0 = TreeNode(weight=0, size=9, depth=0)\n",
    "node1 = TreeNode(weight=1, parent=node0, size=3, depth=1)\n",
    "node2 = TreeNode(weight=2, parent=node1, size=2, depth=2)\n",
    "node3 = TreeNode(weight=3, parent=node2, size=1, depth=3)\n",
    "node4 = TreeNode(weight=4, parent=node0, size=4, depth=1)\n",
    "node5 = TreeNode(weight=5, parent=node0, size=1, depth=1)\n",
    "node6 = TreeNode(weight=6, parent=node4, size=2, depth=2)\n",
    "node7 = TreeNode(weight=7, parent=node4, size=1, depth=2)\n",
    "node8 = TreeNode(weight=8, parent=node6, size=1, depth=3)\n",
    "\n",
    "node0.children = [node1, node4, node5]\n",
    "node1.children = [node2]\n",
    "node2.children = [node3]\n",
    "node4.children = [node6, node7]\n",
    "node6.children = [node8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, None, 9)], [(1, 0, 3), (4, 0, 4), (5, 0, 1)], [(2, 1, 2), (6, 4, 2), (7, 4, 1)], [(3, 2, 1), (8, 6, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# prints level-by-level in tree, from the root downwards, (node.weight, node.parent.weight, node.size)\n",
    "level_traversal(node0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "d = true_distances_all_pairs(node0)\n",
    "print(d[(node3, node5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.012106116823535\n"
     ]
    }
   ],
   "source": [
    "d = basic_distances_all_pairs(node0, node0.size, epsilon=1)\n",
    "print(d[(node3, node5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0421586063295\n"
     ]
    }
   ],
   "source": [
    "d = synthetic_distances_all_pairs(node0, node0.size, epsilon=1)\n",
    "print(d[(node3, node5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.829592219262501\n"
     ]
    }
   ],
   "source": [
    "d = sealfon_distances_all_pairs(node0, node0.size, epsilon=1)\n",
    "print(d[(node3, node5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.413415161902327\n"
     ]
    }
   ],
   "source": [
    "d = fanli_distances_all_pairs(node0, node0.size, epsilon=1)\n",
    "print(d[(node3, node5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and comparison of private all-pairs distances algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dp = {0: basic_distances_all_pairs, 1: synthetic_distances_all_pairs, 2: sealfon_distances_all_pairs, 3: fanli_distances_all_pairs}\n",
    "names = {basic_distances_all_pairs: \"Basic\", synthetic_distances_all_pairs: \"Synthetic\", sealfon_distances_all_pairs: \"Sealfon\", fanli_distances_all_pairs: \"Fan and Li\"}\n",
    "trials = 3\n",
    "\n",
    "def compare_and_plot(n, B, Vs, epsilon, generate):\n",
    "    # average maximum amount an edge weight differs between DP answer and true answer\n",
    "    max_errors = [[] for _ in range(2)]\n",
    "\n",
    "    for V in Vs:\n",
    "        if V % 100 == 0:\n",
    "            print(V)\n",
    "        max_errors_trial = [[float('-inf') for _ in range(trials)] for _ in range(2)]\n",
    "\n",
    "        for i in range(trials):\n",
    "            t = generate(n, B, V)\n",
    "            true_d = true_distances_all_pairs(t)\n",
    "            DP_d = []\n",
    "            for j in range(1, 3):\n",
    "                DP_d.append(dp[j](t, t.size, epsilon=epsilon))\n",
    "\n",
    "            for (u, v) in true_d:\n",
    "                for j in range(1, 3):\n",
    "                    max_errors_trial[j-1][i] = max(max_errors_trial[j-1][i], abs(true_d[(u, v)] - DP_d[j-1][(u, v)]))\n",
    "\n",
    "        for i in range(2):\n",
    "            max_errors_trial[i] = sum(max_errors_trial[i])/len(max_errors_trial[i])\n",
    "            max_errors[i].append(max_errors_trial[i])\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        plt.plot(Vs, max_errors[i-1], label=names[dp[i]])\n",
    "    plt.xlabel(\"Number of Vertices\")\n",
    "    plt.ylabel(\"Average Max Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base versions\n",
    "# compare_and_plot(n=2, B=1000, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "# compare_and_plot(n=4, B=1000, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "\n",
    "# # Varying epsilon\n",
    "# compare_and_plot(n=2, B=1000, Vs=range(1, 501), epsilon=0.1, generate=generate_complete_tree)\n",
    "# compare_and_plot(n=2, B=1000, Vs=range(1, 501), epsilon=10, generate=generate_complete_tree)\n",
    "\n",
    "# # Varying n-ary tree\n",
    "# compare_and_plot(n=2, B=1000, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "# compare_and_plot(n=8, B=1000, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "# compare_and_plot(n=12, B=1000, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "\n",
    "# # Varying weight bounds B\n",
    "# compare_and_plot(n=4, B=100, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "# compare_and_plot(n=4, B=10000, Vs=range(1, 501), epsilon=1, generate=generate_complete_tree)\n",
    "\n",
    "# # Testing path graphs\n",
    "# compare_and_plot(n=None, B=1000, Vs=range(1, 501), epsilon=1, generate=generate_path_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "# Try larger path graphs\n",
    "compare_and_plot(n=None, B=1000, Vs=range(1, 1001), epsilon=1, generate=generate_path_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
